{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93eef2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "zipllm_dir = \"../HF_storage\"          # HF_storage dir\n",
    "fastcdc_dir = \"../experiment/results/fastcdc_chunks\"     # FastCDC results dir\n",
    "zipnn_file = \"../experiment/results/zipnn_results.json\"  # ZipNN results file\n",
    "output_file = \"./plots/ddr_comparison.png\"          # output image file\n",
    "\n",
    "# Ensure the output directory exists for output_file\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266add34",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "668e9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "def process_zipllm_results(storage_dir: str) -> List[Tuple[int, float]]:\n",
    "    model_metadata_dir = os.path.join(storage_dir, \"model_metadata\")\n",
    "    model_files = sorted(os.listdir(model_metadata_dir))\n",
    "    \n",
    "    unique_tensors = set()\n",
    "    total_original_size = 0\n",
    "    unique_tensors_size = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, model_file in enumerate(model_files, 1):\n",
    "        model_path = os.path.join(model_metadata_dir, model_file)\n",
    "        with open(model_path, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "            \n",
    "        model_tensors = set()\n",
    "        model_size = 0\n",
    "        \n",
    "        for file_name, file_hash in model_data[\"files\"].items():\n",
    "            file_metadata_path = os.path.join(storage_dir, \"file_metadata\", f\"{file_hash}.json\")\n",
    "            \n",
    "            with open(file_metadata_path, 'r') as f:\n",
    "                file_data = json.load(f)\n",
    "            model_size += file_data[\"size\"]\n",
    "            for tensor_name, tensor_hash in file_data.get(\"tensor_hashes\", {}).items():\n",
    "                model_tensors.add(tensor_hash)\n",
    "        \n",
    "        total_original_size += model_size\n",
    "        new_tensors = model_tensors - unique_tensors\n",
    "        unique_tensors.update(new_tensors)\n",
    "        \n",
    "        for tensor_hash in new_tensors:\n",
    "            tensor_metadata_path = os.path.join(storage_dir, \"tensor_metadata\", f\"{tensor_hash}.json\")\n",
    "            try:\n",
    "                with open(tensor_metadata_path, 'r') as f:\n",
    "                    tensor_data = json.load(f)\n",
    "                    \n",
    "                compressed_metadata_path = os.path.join(storage_dir, \"compressed_metadata\", f\"{tensor_hash}.json\")\n",
    "                with open(compressed_metadata_path, 'r') as f:\n",
    "                    compressed_data = json.load(f)\n",
    "                    \n",
    "                unique_tensors_size += compressed_data[\"compressed_size\"]\n",
    "            except (FileNotFoundError, json.JSONDecodeError):\n",
    "                continue\n",
    "        \n",
    "        ddr = 1 - (unique_tensors_size / total_original_size)\n",
    "        results.append((i, ddr))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_fastcdc_results(fastcdc_dir: str) -> List[Tuple[int, float]]:\n",
    "    model_dirs = sorted(os.listdir(fastcdc_dir))\n",
    "    \n",
    "    unique_chunks = set()\n",
    "    total_chunk_size = 0\n",
    "    unique_chunks_size = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, model_dir in enumerate(model_dirs, 1):\n",
    "        model_path = os.path.join(fastcdc_dir, model_dir)\n",
    "        chunk_files = glob.glob(os.path.join(model_path, \"*.chunk.txt\"))\n",
    "        \n",
    "        model_chunks = set()\n",
    "        model_chunk_size = 0\n",
    "        \n",
    "        for chunk_file in chunk_files:\n",
    "            with open(chunk_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 2:\n",
    "                        chunk_hash, chunk_size = parts[0], int(parts[1])\n",
    "                        model_chunks.add(chunk_hash)\n",
    "                        model_chunk_size += chunk_size\n",
    "        \n",
    "        total_chunk_size += model_chunk_size\n",
    "        new_chunks = model_chunks - unique_chunks\n",
    "        unique_chunks.update(new_chunks)\n",
    "        \n",
    "        for chunk_file in chunk_files:\n",
    "            with open(chunk_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 2:\n",
    "                        chunk_hash, chunk_size = parts[0], int(parts[1])\n",
    "                        if chunk_hash in new_chunks:\n",
    "                            unique_chunks_size += chunk_size\n",
    "        \n",
    "        ddr = 1 - (unique_chunks_size / total_chunk_size)\n",
    "        results.append((i, ddr))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_zipnn_results(zipnn_file: str) -> List[Tuple[int, float]]:\n",
    "    with open(zipnn_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Sort models to ensure consistent order\n",
    "    models = sorted(data.keys())\n",
    "    \n",
    "    total_original_size = 0\n",
    "    total_compressed_size = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, model in enumerate(models, 1):\n",
    "        model_data = data[model]\n",
    "        \n",
    "        for file, sizes in model_data.items():\n",
    "            total_original_size += sizes[\"original_size\"]\n",
    "            total_compressed_size += sizes[\"compressed_size\"]\n",
    "        \n",
    "        # Calculate DDR: saved bytes / original total size\n",
    "        ddr = 1 - (total_compressed_size / total_original_size)\n",
    "        results.append((i, ddr))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_file_dedup_results(storage_dir: str) -> List[Tuple[int, float]]:\n",
    "    # Get all models\n",
    "    model_metadata_dir = os.path.join(storage_dir, \"model_metadata\")\n",
    "    model_files = sorted(os.listdir(model_metadata_dir))\n",
    "    \n",
    "    # Track unique files and their sizes\n",
    "    unique_files = set()\n",
    "    total_original_size = 0\n",
    "    unique_files_size = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, model_file in enumerate(model_files, 1):\n",
    "        model_path = os.path.join(model_metadata_dir, model_file)\n",
    "        with open(model_path, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "            \n",
    "        model_files_set = set()\n",
    "        model_size = 0\n",
    "        \n",
    "        # Get all files for this model\n",
    "        for file_name, file_hash in model_data[\"files\"].items():\n",
    "            model_files_set.add(file_hash)\n",
    "            \n",
    "            file_metadata_path = os.path.join(storage_dir, \"file_metadata\", f\"{file_hash}.json\")\n",
    "            with open(file_metadata_path, 'r') as f:\n",
    "                file_data = json.load(f)\n",
    "            \n",
    "            # Add file size to model size\n",
    "            model_size += file_data[\"size\"]\n",
    "        \n",
    "        # Add to total original size\n",
    "        total_original_size += model_size\n",
    "        \n",
    "        # Add new unique files\n",
    "        new_files = model_files_set - unique_files\n",
    "        unique_files.update(new_files)\n",
    "        \n",
    "        # Calculate size of unique files\n",
    "        for file_hash in new_files:\n",
    "            file_metadata_path = os.path.join(storage_dir, \"file_metadata\", f\"{file_hash}.json\")\n",
    "            with open(file_metadata_path, 'r') as f:\n",
    "                file_data = json.load(f)\n",
    "                \n",
    "            unique_files_size += file_data[\"size\"]\n",
    "        \n",
    "        # Calculate DDR: saved bytes / original total size\n",
    "        ddr = 1 - (unique_files_size / total_original_size)\n",
    "        results.append((i, ddr))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_results(zipllm_results, fastcdc_results, zipnn_results, file_dedup_results, output_file):\n",
    "    plt.rcParams['figure.figsize'] = (25, 12)\n",
    "    plt.rcParams['xtick.major.width'] = 2\n",
    "    plt.rcParams['ytick.major.width'] = 2\n",
    "    plt.rcParams['axes.grid'] = False\n",
    "    plt.rcParams['grid.linestyle'] = '-'\n",
    "    plt.rcParams['grid.linewidth'] = 1\n",
    "    plt.rcParams['grid.color'] = '#e1e1e1'\n",
    "    plt.rcParams['axes.linewidth'] = 2\n",
    "    plt.rcParams['ytick.major.size'] = 12\n",
    "    plt.rcParams['xtick.major.size'] = 12\n",
    "    plt.rcParams['axes.titlesize'] = 52\n",
    "    plt.rcParams['axes.labelsize'] = 52\n",
    "    plt.rcParams['lines.linewidth'] = 4\n",
    "    plt.rcParams['lines.markersize'] = 30\n",
    "    plt.rcParams['xtick.labelsize'] = 52\n",
    "    plt.rcParams['ytick.labelsize'] = 52\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.rcParams['font.serif'] = ['DejaVu Serif']\n",
    "    plt.rcParams['font.weight'] = 'normal'\n",
    "    plt.rcParams['axes.labelweight'] = 'normal'\n",
    "    plt.rcParams['axes.titleweight'] = 'normal'\n",
    "    plt.rcParams['axes.grid.axis'] = 'both'\n",
    "    plt.rcParams['axes.grid.which'] = 'major'\n",
    "    plt.rcParams['figure.dpi'] = 600\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    if file_dedup_results:\n",
    "        x_file, y_file = zip(*file_dedup_results)\n",
    "        line1, = plt.plot(x_file, y_file, color=colors[0], marker='', label='File Dedup')\n",
    "        plt.text(x_file[-1] + 0.1, y_file[-1], f'{y_file[-1]:.2f}', fontsize=40, va='center')\n",
    "    \n",
    "    if fastcdc_results:\n",
    "        x_fastcdc, y_fastcdc = zip(*fastcdc_results)\n",
    "        line2, = plt.plot(x_fastcdc, y_fastcdc, color=colors[1], marker='', label='HF(FastCDC)')\n",
    "        plt.text(x_fastcdc[-1] + 0.1, y_fastcdc[-1], f'{y_fastcdc[-1]:.2f}', fontsize=40, va='center')\n",
    "    \n",
    "    if zipnn_results:\n",
    "        x_zipnn, y_zipnn = zip(*zipnn_results)\n",
    "        line3, = plt.plot(x_zipnn, y_zipnn, color=colors[2], marker='', label='ZipNN')\n",
    "        plt.text(x_zipnn[-1] + 0.1, y_zipnn[-1], f'{y_zipnn[-1]:.2f}', fontsize=40, va='center')\n",
    "    \n",
    "    if zipllm_results:\n",
    "        x_zipllm, y_zipllm = zip(*zipllm_results)\n",
    "        line4, = plt.plot(x_zipllm, y_zipllm, color=colors[3], marker='', label='ZipLLM')\n",
    "        plt.text(x_zipllm[-1] + 0.1, y_zipllm[-1], f'{y_zipllm[-1]:.2f}', fontsize=40, va='center')\n",
    "    \n",
    "    # Add 50% reduction reference line\n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.text(1, 0.45, '50% Reduction', fontsize=35, va='bottom',color='gray')\n",
    "    \n",
    "    plt.xlabel('Model Count')\n",
    "    plt.ylabel('Data Reduction Ratio')\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    max_models = max(\n",
    "        len(zipllm_results) if zipllm_results else 0,\n",
    "        len(fastcdc_results) if fastcdc_results else 0,\n",
    "        len(zipnn_results) if zipnn_results else 0,\n",
    "        len(file_dedup_results) if file_dedup_results else 0\n",
    "    )\n",
    "    plt.xticks(range(1, max_models + 1))\n",
    "    \n",
    "    plt.legend(loc='upper left', fontsize=40)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    print(f\"Plot saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b0dd424",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../experiment/results/fastcdc_chunks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m zipllm_results \u001b[38;5;241m=\u001b[39m process_zipllm_results(zipllm_dir)\n\u001b[0;32m----> 2\u001b[0m fastcdc_results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_fastcdc_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfastcdc_dir\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m      3\u001b[0m zipnn_results \u001b[38;5;241m=\u001b[39m process_zipnn_results(zipnn_file)\n\u001b[1;32m      4\u001b[0m file_dedup_results \u001b[38;5;241m=\u001b[39m process_file_dedup_results(zipllm_dir)\n",
      "Cell \u001b[0;32mIn[46], line 62\u001b[0m, in \u001b[0;36mprocess_fastcdc_results\u001b[0;34m(fastcdc_dir)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_fastcdc_results\u001b[39m(fastcdc_dir: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m---> 62\u001b[0m     model_dirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfastcdc_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m     unique_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     65\u001b[0m     total_chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../experiment/results/fastcdc_chunks'"
     ]
    }
   ],
   "source": [
    "zipllm_results = process_zipllm_results(zipllm_dir)\n",
    "fastcdc_results = process_fastcdc_results(fastcdc_dir)  \n",
    "zipnn_results = process_zipnn_results(zipnn_file)\n",
    "file_dedup_results = process_file_dedup_results(zipllm_dir)\n",
    "plot_results(zipllm_results, fastcdc_results, zipnn_results, file_dedup_results, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Any\n",
    "\n",
    "def analyze_fastcdc_chunks(fastcdc_dir: str) -> Dict[str, Any]:\n",
    "    print(\"\\n===== FastCDC Chunk Deduplication Statistics =====\")\n",
    "    model_dirs = sorted(os.listdir(fastcdc_dir))\n",
    "    all_chunks = []\n",
    "    unique_chunks = set()\n",
    "    chunk_sizes = {}\n",
    "    for model_dir in model_dirs:\n",
    "        model_path = os.path.join(fastcdc_dir, model_dir)\n",
    "        chunk_files = glob.glob(os.path.join(model_path, \"*.chunk.txt\"))\n",
    "        for chunk_file in chunk_files:\n",
    "            with open(chunk_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 2:\n",
    "                        chunk_hash, chunk_size = parts[0], int(parts[1])\n",
    "                        all_chunks.append(chunk_hash)\n",
    "                        unique_chunks.add(chunk_hash)\n",
    "                        chunk_sizes[chunk_hash] = chunk_size\n",
    "    total_chunks = len(all_chunks)\n",
    "    unique_chunk_count = len(unique_chunks)\n",
    "    chunk_size_values = list(chunk_sizes.values())\n",
    "    max_chunk_size = max(chunk_size_values) if chunk_size_values else 0\n",
    "    avg_chunk_size = sum(chunk_size_values) / len(chunk_size_values) if chunk_size_values else 0\n",
    "    print(f\"Total chunks: {total_chunks}\")\n",
    "    print(f\"Unique chunks: {unique_chunk_count}\")\n",
    "    print(f\"Unique ratio: {unique_chunk_count / total_chunks:.4f}\")\n",
    "    print(f\"Max chunk size: {max_chunk_size} bytes ({max_chunk_size / 1024 / 1024:.2f} MB)\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} bytes ({avg_chunk_size / 1024 / 1024:.2f} MB)\")\n",
    "    return {\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"unique_chunks\": unique_chunk_count,\n",
    "        \"unique_ratio\": unique_chunk_count / total_chunks if total_chunks else 0,\n",
    "        \"max_chunk_size\": max_chunk_size,\n",
    "        \"avg_chunk_size\": avg_chunk_size\n",
    "    }\n",
    "\n",
    "def analyze_tensor_dedup(storage_dir: str) -> Dict[str, Any]:\n",
    "    print(\"\\n===== Tensor Deduplication Statistics =====\")\n",
    "    model_metadata_dir = os.path.join(storage_dir, \"model_metadata\")\n",
    "    model_files = sorted(os.listdir(model_metadata_dir))\n",
    "    all_tensors = []\n",
    "    unique_tensors = set()\n",
    "    tensor_sizes = {}\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(model_metadata_dir, model_file)\n",
    "        with open(model_path, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        for file_name, file_hash in model_data[\"files\"].items():\n",
    "            file_metadata_path = os.path.join(storage_dir, \"file_metadata\", f\"{file_hash}.json\")\n",
    "            try:\n",
    "                with open(file_metadata_path, 'r') as f:\n",
    "                    file_data = json.load(f)\n",
    "                for tensor_name, tensor_hash in file_data.get(\"tensor_hashes\", {}).items():\n",
    "                    all_tensors.append(tensor_hash)\n",
    "                    unique_tensors.add(tensor_hash)\n",
    "                    if tensor_hash not in tensor_sizes:\n",
    "                        tensor_metadata_path = os.path.join(storage_dir, \"tensor_metadata\", f\"{tensor_hash}.json\")\n",
    "                        try:\n",
    "                            with open(tensor_metadata_path, 'r') as f:\n",
    "                                tensor_data = json.load(f)\n",
    "                                tensor_sizes[tensor_hash] = tensor_data.get(\"original_size\", 0)\n",
    "                        except (FileNotFoundError, json.JSONDecodeError):\n",
    "                            tensor_sizes[tensor_hash] = 0\n",
    "            except (FileNotFoundError, json.JSONDecodeError):\n",
    "                continue\n",
    "    total_tensors = len(all_tensors)\n",
    "    unique_tensor_count = len(unique_tensors)\n",
    "    tensor_size_values = list(tensor_sizes.values())\n",
    "    max_tensor_size = max(tensor_size_values) if tensor_size_values else 0\n",
    "    avg_tensor_size = sum(tensor_size_values) / len(tensor_size_values) if tensor_size_values else 0\n",
    "    print(f\"Total tensors: {total_tensors}\")\n",
    "    print(f\"Unique tensors: {unique_tensor_count}\")\n",
    "    print(f\"Unique ratio: {unique_tensor_count / total_tensors:.4f}\")\n",
    "    print(f\"Max tensor size: {max_tensor_size} bytes ({max_tensor_size / 1024 / 1024:.2f} MB)\")\n",
    "    print(f\"Average tensor size: {avg_tensor_size:.2f} bytes ({avg_tensor_size / 1024 / 1024:.2f} MB)\")\n",
    "    return {\n",
    "        \"total_tensors\": total_tensors,\n",
    "        \"unique_tensors\": unique_tensor_count,\n",
    "        \"unique_ratio\": unique_tensor_count / total_tensors if total_tensors else 0,\n",
    "        \"max_tensor_size\": max_tensor_size,\n",
    "        \"avg_tensor_size\": avg_tensor_size\n",
    "    }\n",
    "\n",
    "def analyze_file_dedup(storage_dir: str) -> Dict[str, Any]:\n",
    "    print(\"\\n===== File Deduplication Statistics =====\")\n",
    "    model_metadata_dir = os.path.join(storage_dir, \"model_metadata\")\n",
    "    model_files = sorted(os.listdir(model_metadata_dir))\n",
    "    all_files = []\n",
    "    unique_files = set()\n",
    "    file_sizes = {}\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(model_metadata_dir, model_file)\n",
    "        with open(model_path, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        for file_name, file_hash in model_data[\"files\"].items():\n",
    "            all_files.append(file_hash)\n",
    "            unique_files.add(file_hash)\n",
    "            if file_hash not in file_sizes:\n",
    "                file_metadata_path = os.path.join(storage_dir, \"file_metadata\", f\"{file_hash}.json\")\n",
    "                try:\n",
    "                    with open(file_metadata_path, 'r') as f:\n",
    "                        file_data = json.load(f)\n",
    "                        file_sizes[file_hash] = file_data.get(\"size\", 0)\n",
    "                except (FileNotFoundError, json.JSONDecodeError):\n",
    "                    file_sizes[file_hash] = 0\n",
    "\n",
    "    total_files = len(all_files)\n",
    "    unique_file_count = len(unique_files)\n",
    "    file_size_values = list(file_sizes.values())\n",
    "    max_file_size = max(file_size_values) if file_size_values else 0\n",
    "    avg_file_size = sum(file_size_values) / len(file_size_values) if file_size_values else 0\n",
    "    total_size_all_files = sum(file_size_values)\n",
    "\n",
    "    print(f\"Total files: {total_files}\")\n",
    "    print(f\"Unique files: {unique_file_count}\")\n",
    "    print(f\"Unique ratio: {unique_file_count / total_files:.4f}\")\n",
    "    print(f\"Max file size: {max_file_size} bytes ({max_file_size / 1024 / 1024:.2f} MB)\")\n",
    "    print(f\"Average file size: {avg_file_size:.2f} bytes ({avg_file_size / 1024 / 1024:.2f} MB)\")\n",
    "    print(f\"Total size of all files (Model Size): {total_size_all_files} bytes ({total_size_all_files / 1024 / 1024 / 1024:.2f} GB)\")\n",
    "\n",
    "    return {\n",
    "        \"total_files\": total_files,\n",
    "        \"unique_files\": unique_file_count,\n",
    "        \"unique_ratio\": unique_file_count / total_files if total_files else 0,\n",
    "        \"max_file_size\": max_file_size,\n",
    "        \"avg_file_size\": avg_file_size,\n",
    "        \"total_size_all_files\": total_size_all_files\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_layer_dedup(storage_dir: str) -> Dict[str, Any]:\n",
    "    print(\"\\n===== Layer Deduplication Statistics =====\")\n",
    "    model_metadata_dir = os.path.join(storage_dir, \"model_metadata\")\n",
    "    model_files = sorted(os.listdir(model_metadata_dir))\n",
    "    all_models_layers = []\n",
    "    tensor_sizes = {}\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(model_metadata_dir, model_file)\n",
    "        with open(model_path, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        model_layers = defaultdict(dict)\n",
    "        non_layer_tensors = {}\n",
    "        for file_name, file_hash in model_data[\"files\"].items():\n",
    "            file_metadata_path = os.path.join(storage_dir, \"file_metadata\", f\"{file_hash}.json\")\n",
    "            try:\n",
    "                with open(file_metadata_path, 'r') as f:\n",
    "                    file_data = json.load(f)\n",
    "                for tensor_name, tensor_hash in file_data.get(\"tensor_hashes\", {}).items():\n",
    "                    if tensor_hash not in tensor_sizes:\n",
    "                        tensor_metadata_path = os.path.join(storage_dir, \"tensor_metadata\", f\"{tensor_hash}.json\")\n",
    "                        try:\n",
    "                            with open(tensor_metadata_path, 'r') as f:\n",
    "                                tensor_data = json.load(f)\n",
    "                                tensor_sizes[tensor_hash] = tensor_data.get(\"original_size\", 0)\n",
    "                        except (FileNotFoundError, json.JSONDecodeError):\n",
    "                            tensor_sizes[tensor_hash] = 0\n",
    "                    parts = tensor_name.split('.')\n",
    "                    if len(parts) >= 3 and parts[0] == 'model' and parts[1] == 'layers' and parts[2].isdigit():\n",
    "                        layer_id = int(parts[2])\n",
    "                        tensor_type = '.'.join(parts[3:])\n",
    "                        model_layers[layer_id][tensor_type] = tensor_hash\n",
    "                    else:\n",
    "                        non_layer_tensors[tensor_name] = tensor_hash\n",
    "            except (FileNotFoundError, json.JSONDecodeError):\n",
    "                continue\n",
    "        all_models_layers.append((model_layers, non_layer_tensors))\n",
    "    unique_layer_structures = {}\n",
    "    for model_layers, non_layer_tensors in all_models_layers:\n",
    "        for layer_id, tensor_dict in model_layers.items():\n",
    "            tensor_types = sorted(tensor_dict.keys())\n",
    "            layer_structure = []\n",
    "            for tensor_type in tensor_types:\n",
    "                tensor_hash = tensor_dict[tensor_type]\n",
    "                layer_structure.append(f\"{tensor_type}:{tensor_hash}\")\n",
    "            layer_structure_str = \"|\".join(layer_structure)\n",
    "            layer_hash = hashlib.sha256(layer_structure_str.encode()).hexdigest()\n",
    "            layer_size = sum(tensor_sizes.get(tensor_dict[tensor_type], 0) for tensor_type in tensor_types)\n",
    "            unique_layer_structures[layer_hash] = (f\"layer_{layer_id}\", layer_size)\n",
    "        for tensor_name, tensor_hash in non_layer_tensors.items():\n",
    "            layer_hash = hashlib.sha256(f\"{tensor_name}:{tensor_hash}\".encode()).hexdigest()\n",
    "            unique_layer_structures[layer_hash] = (tensor_name, tensor_sizes.get(tensor_hash, 0))\n",
    "    total_layers = sum(len(model_layers) + len(non_layer_tensors) for model_layers, non_layer_tensors in all_models_layers)\n",
    "    unique_layer_count = len(unique_layer_structures)\n",
    "    layer_sizes = [size for _, size in unique_layer_structures.values()]\n",
    "    max_layer_size = max(layer_sizes) if layer_sizes else 0\n",
    "    avg_layer_size = sum(layer_sizes) / len(layer_sizes) if layer_sizes else 0\n",
    "    print(f\"Total layers: {total_layers}\")\n",
    "    print(f\"Unique layers: {unique_layer_count}\")\n",
    "    print(f\"Unique ratio: {unique_layer_count / total_layers if total_layers else 0:.4f}\")\n",
    "    print(f\"Max layer size: {max_layer_size} bytes ({max_layer_size / 1024 / 1024:.2f} MB)\")\n",
    "    print(f\"Average layer size: {avg_layer_size:.2f} bytes ({avg_layer_size / 1024 / 1024:.2f} MB)\")\n",
    "    return {\n",
    "        \"total_layers\": total_layers,\n",
    "        \"unique_layers\": unique_layer_count,\n",
    "        \"unique_ratio\": unique_layer_count / total_layers if total_layers else 0,\n",
    "        \"max_layer_size\": max_layer_size,\n",
    "        \"avg_layer_size\": avg_layer_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d150694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze FastCDC chunks\n",
    "chunk_stats = analyze_fastcdc_chunks(fastcdc_dir)\n",
    "# Analyze tensor deduplication\n",
    "tensor_stats = analyze_tensor_dedup(zipllm_dir)\n",
    "# Analyze layer deduplication\n",
    "layer_stats = analyze_layer_dedup(zipllm_dir)\n",
    "# Analyze file deduplication\n",
    "file_stats = analyze_file_dedup(zipllm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2420949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "# ---- Unit helpers ----\n",
    "PB_BYTES = 1024 ** 5\n",
    "GB_BYTES = 1024 ** 3\n",
    "MB_BYTES = 1024 ** 2\n",
    "\n",
    "PROJECTED_HF_MODEL_SIZE_PB = 17.0   # total HF model size\n",
    "METADATA_SIZE_BYTES = 64            # per-object metadata size (e.g., hash+overhead)\n",
    "Experiment_Model_Size_GB = 74.79\n",
    "\n",
    "def compute_and_print(name: str, stats: Dict[str, Any], unique_key: str, avg_size_key: str):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "      - Estimated Metadata (MB): unique_hashes * metadata_size\n",
    "      - Projected HF Metadata (GB): estimated_metadata_mb * (PROJECTED_HF_MODEL_SIZE_PB * 1024 / Experiment_Model_Size_GB)\n",
    "    Then print nicely.\n",
    "    \"\"\"\n",
    "    if not stats:\n",
    "        print(f\"[WARN] No stats for {name}, skip.\")\n",
    "        return\n",
    "\n",
    "    unique_hashes = int(stats.get(unique_key, 0) or 0)\n",
    "    avg_size_bytes = float(stats.get(avg_size_key, 0.0) or 0.0)\n",
    "\n",
    "    if unique_hashes <= 0 or avg_size_bytes <= 0:\n",
    "        print(f\"[WARN] Invalid values for {name}: unique={unique_hashes}, avg_size={avg_size_bytes} bytes; skip.\")\n",
    "        return\n",
    "\n",
    "    # Estimated metadata for the catalog of unique hashes (in MB)\n",
    "    estimated_metadata_mb = (unique_hashes * METADATA_SIZE_BYTES) / MB_BYTES\n",
    "\n",
    "    # Projected metadata if scaled to the whole HF corpus (in GB)\n",
    "    # Formula: estimated_metadata_mb * (PROJECTED_HF_MODEL_SIZE_PB * 1024 / Experiment_Model_Size_GB)\n",
    "    projected_metadata_gb = estimated_metadata_mb * (PROJECTED_HF_MODEL_SIZE_PB * 1024) / Experiment_Model_Size_GB\n",
    "\n",
    "    def human(n):\n",
    "        return f\"{n:,.2f}\"\n",
    "\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"Unique Hashes           : {unique_hashes:,}\")\n",
    "    print(f\"Avg Size                : {avg_size_bytes/MB_BYTES:.3f} MB\")\n",
    "    print(f\"Estimated Metadata (MB) : {human(estimated_metadata_mb)}\")\n",
    "    print(f\"Projected HF Metadata (GB): {human(projected_metadata_gb)}\")\n",
    "\n",
    "\n",
    "compute_and_print(\"ChunkDedup (FastCDC)\", chunk_stats,  \"unique_chunks\",  \"avg_chunk_size\")\n",
    "compute_and_print(\"TensorDedup (ours)\",  tensor_stats, \"unique_tensors\", \"avg_tensor_size\")\n",
    "compute_and_print(\"LayerDedup\",           layer_stats,  \"unique_layers\",  \"avg_layer_size\")\n",
    "compute_and_print(\"FileDedup\",            file_stats,   \"unique_files\",   \"avg_file_size\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
